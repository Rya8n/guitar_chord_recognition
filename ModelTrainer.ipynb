{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"chords/\"\n",
    "TEST_DATASET_PATH = \"chords_test_val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgPathAndLabels(dataset_path):\n",
    "   def listDirsInDir(directory):\n",
    "      directories = []\n",
    "      for root, dirs, _ in os.walk(directory):\n",
    "         for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name).replace(\"\\\\\", \"/\")\n",
    "            directories.append(dir_path)\n",
    "      return directories\n",
    "\n",
    "   def listFilesInDirectory(directory):\n",
    "      files = []\n",
    "      for root, _, filenames in os.walk(directory):\n",
    "         for filename in filenames:\n",
    "            file_path = os.path.join(root, filename).replace(\"\\\\\", \"/\")\n",
    "            files.append(file_path)\n",
    "      return files\n",
    "   \n",
    "   chordTypes = listDirsInDir(dataset_path)\n",
    "   image_paths = []\n",
    "   for chordType in chordTypes:\n",
    "      listOfASingleTypeOfChordDirectories = listFilesInDirectory(chordType)\n",
    "      image_paths.append(listOfASingleTypeOfChordDirectories)\n",
    "   return image_paths,chordTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(img_paths,img_labels):\n",
    "   x_dataset = []\n",
    "   y_dataset = []\n",
    "   with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.1) as hands:\n",
    "      label_id = 0\n",
    "      for specificChord in img_paths:\n",
    "         for idx, file in enumerate(specificChord):\n",
    "            # reading the image\n",
    "            image = cv2.imread(file)\n",
    "            # Convert the BGR image to RGB before processing.\n",
    "            results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            try:\n",
    "               for hand_landmarks in results.multi_hand_landmarks:\n",
    "                  keypoints = []\n",
    "                  for data_point in hand_landmarks.landmark:\n",
    "                     keypoints.append([data_point.x, data_point.y, data_point.z])\n",
    "               x_dataset.append(keypoints)\n",
    "               y_dataset.append(label_id)\n",
    "            except:\n",
    "               pass\n",
    "         label_id = label_id + 1\n",
    "   x_dataset = np.array(x_dataset, dtype=np.float32)\n",
    "   y_dataset = np.array(y_dataset, dtype=np.float32)\n",
    "   return x_dataset, y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randAndSplit(x_dataset, y_dataset, split_ratio): ### pass the amount of training for split_ratio\n",
    "   # Check if the lengths of x_dataset and y_dataset match\n",
    "   assert len(x_dataset) == len(y_dataset), \"Lengths of x_dataset and y_dataset must match\"\n",
    "\n",
    "   # Get the total number of samples\n",
    "   total_samples = len(x_dataset)\n",
    "\n",
    "   # Create indices for shuffling\n",
    "   indices = np.arange(total_samples)\n",
    "   np.random.shuffle(indices)\n",
    "\n",
    "   # Shuffle x_dataset and y_dataset based on the indices\n",
    "   x_dataset_shuffled = x_dataset[indices]\n",
    "   y_dataset_shuffled = y_dataset[indices]\n",
    "\n",
    "   # Calculate the split index based on the split ratio\n",
    "   split_index = int(total_samples * split_ratio)\n",
    "\n",
    "   # Split the datasets into train and test sets\n",
    "   x_train = x_dataset_shuffled[:split_index]\n",
    "   x_test = x_dataset_shuffled[split_index:]\n",
    "   y_train = y_dataset_shuffled[:split_index]\n",
    "   y_test = y_dataset_shuffled[split_index:]\n",
    "\n",
    "   return x_train, x_test, y_train, y_test\n",
    "\n",
    "def stratifiedSplit(x_dataset, y_dataset, test_size): \n",
    "    # Perform a stratified split based on y_dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_dataset, y_dataset, test_size=test_size, stratify=y_dataset\n",
    "    )\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def equalize_data(x_dataset, y_dataset):\n",
    "    unique_labels, label_counts = np.unique(y_dataset, return_counts=True)\n",
    "    min_samples = min(label_counts)\n",
    "    equalized_x = []\n",
    "    equalized_y = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i, y in enumerate(y_dataset) if y == label]\n",
    "        selected_indices = random.sample(label_indices, min_samples)\n",
    "        \n",
    "        equalized_x.extend([x_dataset[i] for i in selected_indices])\n",
    "        equalized_y.extend([y_dataset[i] for i in selected_indices])\n",
    "\n",
    "    return np.array(equalized_x), np.array(equalized_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_arrays(pre_nudge_array, post_nudge_array):\n",
    "    combined_array = np.concatenate((pre_nudge_array, post_nudge_array))\n",
    "    return combined_array\n",
    "\n",
    "def nudge_array(arr, min_nudge, max_nudge):\n",
    "    # Generate random nudge values within the specified range\n",
    "    nudge_values = np.random.uniform(low=min_nudge, high=max_nudge, size=arr.shape)\n",
    "    \n",
    "    # Apply the nudge values to the original array\n",
    "    nudged_array = arr + nudge_values\n",
    "    \n",
    "    return nudged_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class highAccuracyCallback(tf.keras.callbacks.Callback): # just in case\n",
    "   def on_epoch_end(self, epoch, logs={}):\n",
    "      if logs.get('val_accuracy') is not None and logs.get('val_accuracy') > 0.90:\n",
    "         print(\"\\nModel reached morea than 95.0% accuracy. Stopping training\")\n",
    "         self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(21,3)),\n",
    "      tf.keras.layers.Dense(512, activation= 'linear'),\n",
    "      tf.keras.layers.Dense(512, activation= 'linear'),\n",
    "      tf.keras.layers.Dense(256, activation= 'linear'),\n",
    "      tf.keras.layers.Dense(256, activation= 'tanh'),\n",
    "      tf.keras.layers.Dense(128, activation= 'selu'),\n",
    "      tf.keras.layers.Dense(128, activation= 'selu'),\n",
    "      tf.keras.layers.Dense(64, activation= 'selu'),\n",
    "      tf.keras.layers.Dense(7, activation='softmax'),\n",
    "  ])\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSprop(lr=1e-5), metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(loaded_model, x_test, y_test, loaded_labels):\n",
    "    preds = []\n",
    "    for predTargetIdx in range(len(x_test)):\n",
    "        pred = loaded_model.predict(x_test[predTargetIdx:predTargetIdx+1])\n",
    "        preds.append(np.argmax(pred))\n",
    "    preds = np.array(preds)\n",
    "    conf_matrix = confusion_matrix(y_test, preds, labels=range(len(loaded_labels)))\n",
    "    return conf_matrix\n",
    "\n",
    "def confusionMatrixPercentages(conf_matrix, loaded_labels):\n",
    "    total_samples = np.sum(conf_matrix, axis=1)  # Sum of samples per actual class\n",
    "    conf_matrix_percent = np.round((conf_matrix / total_samples[:, None]) * 100, 2)\n",
    "    \n",
    "    # Displaying confusion matrix in percentages\n",
    "    print(\"Confusion Matrix in Percentages:\")\n",
    "    print(\"Actual/Predicted\\t\", end=\"\")\n",
    "    for label in loaded_labels:\n",
    "        print(label, \"\\t\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, row in enumerate(conf_matrix_percent):\n",
    "        print(f\"{loaded_labels[i]}\\t\\t\", end=\"\")\n",
    "        for col in row:\n",
    "            print(f\"{col}%\\t\", \"       \", end=\"\")\n",
    "        print()\n",
    "\n",
    "def calculateAccuracy(conf_matrix):\n",
    "    correct_predictions = np.trace(conf_matrix)\n",
    "    total_samples = np.sum(conf_matrix) \n",
    "    accuracy = correct_predictions / total_samples\n",
    "    accuracy_percent = accuracy * 100\n",
    "    print(f\"Model Accuracy: {accuracy_percent:.2f}%\")\n",
    "\n",
    "def calculateRecall(conf_matrix):\n",
    "    recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    recall_percent = np.round(recall * 100, 2)\n",
    "    average_recall = np.mean(recall)\n",
    "    print(f\"Average Recall: {average_recall * 100:.2f}%\")\n",
    "    return average_recall\n",
    "\n",
    "def calculatePrecision(conf_matrix):\n",
    "    precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
    "    precision_percent = np.round(precision * 100, 2)\n",
    "    average_precision = np.mean(precision)\n",
    "    print(f\"Average Precision: {average_precision * 100:.2f}%\")\n",
    "    return average_precision\n",
    "\n",
    "def calculatePrecisionRecallAndF1Score(conf_matrix):\n",
    "    precision = calculatePrecision(conf_matrix)\n",
    "    recall = calculateRecall(conf_matrix)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_score_percent = np.round(f1_score * 100, 2)\n",
    "    average_f1_score = np.mean(f1_score)\n",
    "    print(f\"Average F1 Score: {average_f1_score * 100:.2f}%\")\n",
    "    return average_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printKptsInConsole(keypoints):\n",
    "   # print the kpts to console\n",
    "   idx=0\n",
    "   while idx != len(keypoints):\n",
    "      print(\"kpt\",idx,keypoints[idx])\n",
    "      idx=idx+1\n",
    "\n",
    "def showInPyplot(keypoints):\n",
    "   # showing it in Pyplot\n",
    "   kptsXpos = []\n",
    "   kptsYpos = []\n",
    "   kptsZpos = []\n",
    "   for kpt in keypoints:\n",
    "      kptsXpos.append(kpt[0])\n",
    "      kptsYpos.append(kpt[1])\n",
    "      kptsZpos.append(kpt[2])\n",
    "\n",
    "   kptsXpos = np.array(kptsXpos)\n",
    "   kptsYpos = np.array(kptsYpos)\n",
    "   kptsZpos = np.array(kptsZpos)\n",
    "\n",
    "   ax = plt.axes(projection='3d')\n",
    "   ax.grid()\n",
    "\n",
    "   #ax.scatter(kptsXpos, kptsYpos, kptsZpos, c = 'r', s = 50)\n",
    "   ax.set_title('3D Scatter Plot')\n",
    "   plt.plot(kptsXpos,kptsYpos,kptsZpos, 'ro')\n",
    "\n",
    "   def connectpoints(x,y,z,p1,p2):\n",
    "      x1, x2 = x[p1], x[p2]\n",
    "      y1, y2 = y[p1], y[p2]\n",
    "      z1, z2 = z[p1], z[p2]\n",
    "      plt.plot([x1,x2],[y1,y2],[z1,z2],'k-')\n",
    "\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,0,1)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,1,2)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,2,3)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,3,4)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,0,5)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,5,6)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,6,7)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,7,8)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,5,9)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,9,10)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,10,11)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,11,12)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,9,13)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,13,14)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,14,15)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,15,16)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,13,17)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,17,18)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,18,19)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,19,20)\n",
    "   connectpoints(kptsXpos,kptsYpos,kptsZpos,0,17)\n",
    "\n",
    "   # Set axes label\n",
    "   ax.set_xlabel('x', labelpad=20)\n",
    "   ax.set_ylabel('y', labelpad=20)\n",
    "   ax.set_zlabel('z', labelpad=20)\n",
    "   #uncomment the code below to make each cell equal in x y and z representation\n",
    "   #plt.axis('equal')\n",
    "   plt.show()\n",
    "\n",
    "def plotTrainingHistory(history):\n",
    "   #-----------------------------------------------------------\n",
    "   # Retrieve a list of list results on training and test data\n",
    "   # sets for each training epoch\n",
    "   #-----------------------------------------------------------\n",
    "   acc=history.history['accuracy']\n",
    "   val_acc=history.history['val_accuracy']\n",
    "   loss=history.history['loss']\n",
    "   val_loss=history.history['val_loss']\n",
    "\n",
    "   epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "   #------------------------------------------------\n",
    "   # Plot training and validation accuracy per epoch\n",
    "   #------------------------------------------------\n",
    "   plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "   plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "   plt.title('Training and validation accuracy')\n",
    "   plt.show()\n",
    "   print(\"\")\n",
    "\n",
    "   #------------------------------------------------\n",
    "   # Plot training and validation loss per epoch\n",
    "   #------------------------------------------------\n",
    "   plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "   plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting images...\")\n",
    "paths,labels = getImgPathAndLabels(DATASET_PATH)\n",
    "paths_test,labels_test = getImgPathAndLabels(TEST_DATASET_PATH)\n",
    "print(\"Getting KPTS...\")\n",
    "x_dataset,y_dataset = getData(paths,labels)\n",
    "x_test, y_test = getData(paths_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating dataset...\")\n",
    "x_train, y_train = equalize_data(x_dataset, y_dataset)\n",
    "x_test, y_test = equalize_data(x_test, y_test)\n",
    "\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "x_test, x_val, y_test, y_val = stratifiedSplit(x_test, y_test, test_size=0.5)\n",
    "\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print(type(x_val))\n",
    "print(type(y_val))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printKptsInConsole(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Augmenting dataset...\")\n",
    "x_train = combine_arrays(x_train,nudge_array(x_train, min_nudge=-0.05, max_nudge=0.05))\n",
    "y_train = combine_arrays(y_train,y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "callbacks = highAccuracyCallback()\n",
    "model = create_model()\n",
    "history = model.fit(x=x_train, y=y_train, epochs=300, validation_data=(x_val,y_val))\n",
    "plotTrainingHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"deltaNET.h5\")\n",
    "pickle.dump(labels, open(\"labels.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"deltaNET.h5\")\n",
    "loaded_labels = pickle.load(open(\"labels.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = testModel(loaded_model,x_test,y_test,loaded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrixPercentages(conf_matrix, loaded_labels)\n",
    "calculateAccuracy(conf_matrix)\n",
    "calculatePrecisionRecallAndF1Score(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
